{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_solve_rate(final_preds,final_tok_types,final_refs):\n",
    "  correct = 0\n",
    "  overall = len(final_preds)\n",
    "  for pred, tt, ref in zip(final_preds,final_tok_types,final_refs):\n",
    "    res_tokens = tf.boolean_mask(pred, tt)\n",
    "    final_answer_start = tf.where(ref == 4242)[0][0]\n",
    "    final_answer_end = tf.where(ref == 50256)[0][0]\n",
    "    real_res = ref[final_answer_start+1:final_answer_end]\n",
    "    if 4242 in res_tokens:\n",
    "      final_answer_start = tf.where(ref == 4242)[0][0]\n",
    "      final_answer_end = tf.where(ref == 50256)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7473it [00:00, 288685.55it/s]\n",
      "1319it [00:00, 224688.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 7473, 'test': 1319}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = '/home/asagirova/gsm8k-t5'\n",
    "nq_tsv_path = {\n",
    "    \"train\": os.path.join(DATA_DIR, \"gsm8k-train.tsv\"),\n",
    "    \"test\": os.path.join(DATA_DIR, \"gsm8k-test.tsv\")\n",
    "}\n",
    "\n",
    "def nq_jsonl_to_tsv(split, out_fname):\n",
    "  count = 0\n",
    "  path = os.path.join(\"/home/asagirova/asc-trr/grade-school-math/grade_school_math/data/\", f\"{split}.jsonl\")\n",
    "  data = read_jsonl(path)\n",
    "   \n",
    "  with open(out_fname, \"w\") as outfile:\n",
    "    for i, ex in tqdm(enumerate(data)):\n",
    "      \n",
    "      # Questions in NQ do not include a question mark.\n",
    "      _id = i\n",
    "      question = ex[\"question\"] + \"\\n\"\n",
    "      solution, result = ex[\"answer\"].split(\"####\")\n",
    "      solution = \"<|startoftext|>\" + solution\n",
    "      result = \"####\" + result + \"<|endoftext|>\"\n",
    "      \n",
    "      outfile.write(\"%s\\t%s\\t%s\\t%s\\n\" % (_id, question, solution, result))\n",
    "      count += 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "\n",
    "# Create TSVs and get counts.\n",
    "num_nq_examples = {}\n",
    "num_nq_examples['train'] = nq_jsonl_to_tsv(\n",
    "      'train', nq_tsv_path['train'])\n",
    "num_nq_examples['test'] = nq_jsonl_to_tsv(\n",
    "      'test', nq_tsv_path['test'])\n",
    "\n",
    "num_nq_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'####'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<t5.data.dataset_providers.FunctionTask at 0x7f4feac1e5e0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from t5.data.tasks import DEFAULT_OUTPUT_FEATURES\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import t5\n",
    "\n",
    "from tqdm import tqdm\n",
    "from t5 import seqio\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def read_jsonl(path: str):\n",
    "    with open(path) as fh:\n",
    "        return [json.loads(line) for line in fh.readlines() if line]\n",
    "\n",
    "\n",
    "def get_examples(split):\n",
    "    path = os.path.join(\"/home/asagirova/asc-trr/grade-school-math/grade_school_math/data/\", f\"{split}.jsonl\")\n",
    "    examples = read_jsonl(path)\n",
    "    for i, ex in enumerate(examples):\n",
    "        ex.update(_id=str(i))\n",
    "        ex.update(question=ex[\"question\"] + \"\\n\")\n",
    "        \n",
    "        solutn, res = ex[\"answer\"].split(\"####\")\n",
    "        ex.update(solution=solutn) #\"<|startoftext|>\" + \n",
    "        ex.update(result=\"####\"+res#+\"<|endoftext|>\"\n",
    "                 )\n",
    "    print(f\"{len(examples)} {split} examples\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "DATA_DIR = '/home/asagirova/gsm8k-t5'\n",
    "nq_tsv_path = {\n",
    "    \"train\": os.path.join(DATA_DIR, \"gsm8k-train.tsv\"),\n",
    "    \"test\": os.path.join(DATA_DIR, \"gsm8k-test.tsv\")\n",
    "}\n",
    "\n",
    "def nq_jsonl_to_tsv(split, out_fname):\n",
    "  count = 0\n",
    "  path = os.path.join(\"/home/asagirova/asc-trr/grade-school-math/grade_school_math/data/\", f\"{split}.jsonl\")\n",
    "  data = read_jsonl(path)\n",
    "   \n",
    "  with open(out_fname, \"w\") as outfile:\n",
    "    for i, ex in tqdm(enumerate(data)):\n",
    "      \n",
    "      # Questions in NQ do not include a question mark.\n",
    "      _id = i\n",
    "      question = ex[\"question\"] + \"\\n\"\n",
    "      solution, result = ex[\"answer\"].split(\"####\")\n",
    "      solution = \"<|startoftext|>\" + solution\n",
    "      result = \"####\" + result + \"<|endoftext|>\"\n",
    "      \n",
    "      outfile.write(\"%s\\t%s\\t%s\\t%s\\n\" % (_id, question, solution, result))\n",
    "      count += 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "\n",
    "# Create TSVs and get counts.\n",
    "num_nq_examples = {}\n",
    "num_nq_examples['train'] = nq_jsonl_to_tsv(\n",
    "      os.path.join(DATA_DIR, 'hotpot_train_v1.1.json'), nq_tsv_path['train'])\n",
    "num_nq_examples['validation'] = nq_jsonl_to_tsv(\n",
    "      os.path.join(DATA_DIR, 'hotpot_dev_distractor_v1.json'), nq_tsv_path['validation'])\n",
    "\n",
    "json.dump(num_nq_examples, tf.io.gfile.GFile(nq_counts_path, \"w\"))\n",
    "'''\n",
    "\n",
    "\n",
    "def nq_dataset_fn(split, shuffle_files=False):\n",
    "  \n",
    "  import functools\n",
    "  import os\n",
    "  import time\n",
    "  import warnings\n",
    "  # We only have one file for each split.\n",
    "  del shuffle_files\n",
    "\n",
    "  \n",
    "  examples = get_examples(split)\n",
    "  ids_str = [ex[\"_id\"] for ex in tqdm(examples)]\n",
    "  qns_str = [ex[\"question\"] for ex in tqdm(examples)]\n",
    "  sol_str = [ex[\"solution\"] for ex in tqdm(examples)]\n",
    "  res_str = [ex[\"result\"] for ex in tqdm(examples)]\n",
    "  ds = tf.data.Dataset.from_generator(lambda: zip(ids_str, qns_str, sol_str, res_str),\n",
    "                                      output_types=(tf.string, tf.string, tf.string, tf.string),\n",
    "                                      output_shapes=((), (), (), ())\n",
    "                                     )\n",
    "  '''\n",
    "  # Load lines from the text file as examples.\n",
    "  ds = tf.data.TextLineDataset(nq_tsv_path[split])\n",
    "  # Split each \"<question>\\t<answer>\" example into (question, answer) tuple.\n",
    "  ds = ds.map(\n",
    "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\", \"\", \"\"],\n",
    "                        field_delim=\"\\t\", use_quote_delim=False),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  '''\n",
    "  # Map each tuple to a {\"question\": ... \"answer\": ...} dict.\n",
    "  ds = ds.map(lambda *ex: dict(zip([\"id\", \"question\", \"solution\", \"result\"], ex)))\n",
    "  ds = ds.map(lambda ex: {\"id\": ex['id'], \"question\": ex['question'], \"solution\": ex['solution'], \"result\": ex['result']})\n",
    "  return ds\n",
    "\n",
    "\n",
    "@seqio.map_over_dataset\n",
    "def gsm8k(x, include_context=False):\n",
    "  from t5.data.preprocessors import _string_join\n",
    "  \"\"\"Convert GSM examples to a text2text pair.\n",
    "  GSM produces examples with this form:\n",
    "    {'id': <id>, 'question': <problem>, solution': <detailed solution>,\n",
    "     'result': <resulting answer>}\n",
    "  This function will return examples of the format:\n",
    "    {'inputs': 'question: <question> solution: <article>',\n",
    "     'targets': '<result>',\n",
    "     'id': <id>, 'question': <question>, 'solution': <solution>,\n",
    "     'result': <result>},\n",
    "  Args:\n",
    "    x: an example to process.\n",
    "    include_context: a boolean\n",
    "  Returns:\n",
    "    A preprocessed example with the format listed above.\n",
    "  \"\"\"\n",
    "  def _gsm_pad_punctuation(text):\n",
    "    \"\"\"Adds spaces around punctuation.\"\"\"\n",
    "    # Add space around punctuation.\n",
    "    text = tf.strings.regex_replace(text, r'([^0-9A-Za-z#_])', r' \\1 ')\n",
    "    # Collapse consecutive whitespace into one space.\n",
    "    text = tf.strings.regex_replace(text, r'\\s+', ' ')\n",
    "    return text\n",
    "  \n",
    "  q = _gsm_pad_punctuation(x['question'])\n",
    "  s = _gsm_pad_punctuation(x['solution'])\n",
    "  r = _gsm_pad_punctuation(x['result'])\n",
    "  if include_context:\n",
    "    inputs = _string_join(['question:', q, 'solution:', s])\n",
    "    targets = r\n",
    "  else:\n",
    "    inputs = _string_join(['question:', q])\n",
    "    targets = s + r\n",
    "  return {\n",
    "      'inputs': inputs,\n",
    "      'targets': targets,\n",
    "      'id': x['id'],\n",
    "      'question': q,\n",
    "      'solution': s,\n",
    "      'result': r\n",
    "  }\n",
    "\n",
    "#print(\"A few raw validation examples...\")\n",
    "#for ex in tfds.as_numpy(nq_dataset_fn(\"train\").take(5)):\n",
    "#  print(ex)\n",
    "\n",
    "'''\n",
    "def trivia_preprocessor(ds):\n",
    "  def normalize_text(text):\n",
    "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
    "    return text\n",
    "\n",
    "  def to_inputs_and_targets(ex):\n",
    "    \"\"\"Map {\"question\": ..., \"answer\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
    "    return {\n",
    "        \"inputs\":\n",
    "             tf.strings.join(#'question: <question> context: <article>'\n",
    "                 [\"question: \", normalize_text(ex[\"question\"]),\"context: \", normalize_text(ex[\"context\"])]),\n",
    "        \"targets\": normalize_text(ex[\"answer\"])\n",
    "    }\n",
    "  return ds.map(to_inputs_and_targets, \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                \n",
    "\n",
    "\n",
    "from create_hotpotqa_task import nq_dataset_fn, num_nq_examples, trivia_preprocessor\n",
    "t5.data.TaskRegistry.add(\n",
    "    \"hotpotqa_context\",\n",
    "    # Specify the task type.\n",
    "    t5.data.Task,\n",
    "    # Supply a function which returns a tf.data.Dataset.\n",
    "    dataset_fn=nq_dataset_fn,\n",
    "    splits=[\"train\"],\n",
    "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
    "    text_preprocessor=[trivia_preprocessor],\n",
    "    # Lowercase targets before computing metrics.\n",
    "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
    "    # We'll use accuracy as our evaluation metric.\n",
    "    metric_fns=[t5.evaluation.metrics.accuracy]#,\n",
    "    # Not required, but helps for mixing and auto-caching.\n",
    "    #num_input_examples=num_nq_examples\n",
    ")\n",
    "\n",
    "nq_task = t5.data.TaskRegistry.get(\"hotpotqa_context\")\n",
    "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 128, \"targets\": 32})\n",
    "'''\n",
    "#print(\"A few preprocessed validation examples...\")\n",
    "#for ex in tfds.as_numpy(ds.take(5)):\n",
    "#  print(ex)\n",
    "def test_solve_rate(targets, predictions):\n",
    "  \"\"\"Computes SQuAD metrics, maximizing over answers per question.\n",
    "  Args:\n",
    "    targets: list of lists of results\n",
    "    predictions: list of predicted solutions\n",
    "  Returns:\n",
    "    dict with score_key: squad score across all targets and predictions\n",
    "  \"\"\"\n",
    "  targets = [qa_utils.normalize_squad(t)for t in targets]\n",
    "  predictions = [qa_utils.normalize_squad(p) for p in predictions]\n",
    "  \n",
    "  if len(targets) != len(predictions):\n",
    "    raise ValueError(\"Number of targets and predictions must match.\")\n",
    "  overall = len(targets)\n",
    "  c = 0\n",
    "  for t, p in zip(targets, predictions):\n",
    "    \n",
    "    # t5.data.get_default_vocabulary().decode([3, 30345,30345]) = \"####\"\n",
    "    final_answer_start = tf.where(t == 30345)[-1][0]\n",
    "    final_answer_end = tf.where(t == 1)[0][0]\n",
    "    real_res = t[final_answer_start+1:final_answer_end]\n",
    "  \n",
    "    \n",
    "    zz = tf.where(p == 30345)\n",
    "    if zz:\n",
    "      pred_answer_start = zz[-1][0]\n",
    "      ozzy = tf.where(p == 1)\n",
    "      if ozzy:\n",
    "        pred_answer_end = ozzy[0][0]\n",
    "        pred_res = p[pred_answer_start+1:pred_answer_end]\n",
    "        c += int(real_res == pred_res)\n",
    "      else:\n",
    "        logging.info(f'/n/npred answer has no eos token: {p}/n/n')\n",
    "    else:\n",
    "        logging.info(f'/n/npred answer has no #### token: {p}/n/n')\n",
    "        \n",
    "  \n",
    "  tsr = c / overall\n",
    "  tsr *= 100\n",
    "  logging.info(\"Test Solve Rate = %.2f\", tsr)\n",
    "  return {\"tsr\": tsr}\n",
    "\n",
    "\n",
    "t5.data.TaskRegistry.add(\n",
    "    \"gsm8k_baseline\",\n",
    "    # Specify the task type.\n",
    "    t5.data.Task,\n",
    "    # Supply a function which returns a tf.data.Dataset.\n",
    "    dataset_fn=nq_dataset_fn,\n",
    "    splits=[\"train\", \"test\"],\n",
    "    text_preprocessor=[\n",
    "        functools.partial(gsm8k, include_context=True),\n",
    "    ],\n",
    "    postprocess_fn=t5.data.postprocessors.qa,\n",
    "    metric_fns=[t5.evaluation.metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "t5.data.TaskRegistry.add(\n",
    "    \"gsm8k_wm\",\n",
    "    # Specify the task type.\n",
    "    t5.data.Task,\n",
    "    # Supply a function which returns a tf.data.Dataset.\n",
    "    dataset_fn=nq_dataset_fn,\n",
    "    splits=[\"train\", \"test\"],\n",
    "    text_preprocessor=[\n",
    "        functools.partial(gsm8k, include_context=False),\n",
    "    ],\n",
    "    postprocess_fn=t5.data.postprocessors.qa,\n",
    "    metric_fns=[t5.evaluation.metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473 train examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7473/7473 [00:00<00:00, 2333013.31it/s]\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 2236622.93it/s]\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 2096030.08it/s]\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 2696724.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 532, 12)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = t5.data.TaskRegistry.get(\"gsm8k_baseline\").get_dataset(split=\"train\", sequence_length={\"inputs\": 1024, \"targets\": 1024})\n",
    "sum([len(i['inputs']) > 512 for i in ds]), max([len(i['inputs']) for i in ds]), max([len(i['targets']) for i in ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1319 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1319/1319 [00:00<00:00, 2217349.49it/s]\n",
      "100%|██████████| 1319/1319 [00:00<00:00, 2010278.70it/s]\n",
      "100%|██████████| 1319/1319 [00:00<00:00, 2030197.06it/s]\n",
      "100%|██████████| 1319/1319 [00:00<00:00, 2287014.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 538, 12)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = t5.data.TaskRegistry.get(\"gsm8k_baseline\").get_dataset(split=\"test\", sequence_length={\"inputs\": 1024, \"targets\": 1024})\n",
    "sum([len(i['inputs']) > 512 for i in ds]), max([len(i['inputs']) for i in ds]), max([len(i['targets']) for i in ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/asagirova/hotpotqa/hotpot_train_v1.1.json\", 'r') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<t5.data.dataset_providers.FunctionTask at 0x7f1725ee9fd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from t5.data.tasks import DEFAULT_OUTPUT_FEATURES\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import t5\n",
    "\n",
    "from tqdm import tqdm\n",
    "from t5 import seqio\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def read_jsonl(path: str):\n",
    "    with open(path) as fh:\n",
    "        return [json.loads(line) for line in fh.readlines() if line]\n",
    "\n",
    "\n",
    "def get_examples(split):\n",
    "    path = os.path.join(\"/home/asagirova/asc-trr/grade-school-math/grade_school_math/data/\", f\"{split}.jsonl\")\n",
    "    examples = read_jsonl(path)\n",
    "    for i, ex in enumerate(examples):\n",
    "        ex.update(_id=str(i))\n",
    "        ex.update(question=ex[\"question\"] + \"\\n\")\n",
    "        \n",
    "        solutn, res = ex[\"answer\"].split(\"####\")\n",
    "        ex.update(solution=solutn) #\"<|startoftext|>\" + \n",
    "        ex.update(result=\"####\"+res#+\"<|endoftext|>\"\n",
    "                 )\n",
    "    print(f\"{len(examples)} {split} examples\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "DATA_DIR = '/home/asagirova/gsm8k-t5'\n",
    "nq_tsv_path = {\n",
    "    \"train\": os.path.join(DATA_DIR, \"gsm8k-train.tsv\"),\n",
    "    \"test\": os.path.join(DATA_DIR, \"gsm8k-test.tsv\")\n",
    "}\n",
    "\n",
    "def nq_jsonl_to_tsv(split, out_fname):\n",
    "  count = 0\n",
    "  path = os.path.join(\"/home/asagirova/asc-trr/grade-school-math/grade_school_math/data/\", f\"{split}.jsonl\")\n",
    "  data = read_jsonl(path)\n",
    "   \n",
    "  with open(out_fname, \"w\") as outfile:\n",
    "    for i, ex in tqdm(enumerate(data)):\n",
    "      \n",
    "      # Questions in NQ do not include a question mark.\n",
    "      _id = i\n",
    "      question = ex[\"question\"] + \"\\n\"\n",
    "      solution, result = ex[\"answer\"].split(\"####\")\n",
    "      solution = \"<|startoftext|>\" + solution\n",
    "      result = \"####\" + result + \"<|endoftext|>\"\n",
    "      \n",
    "      outfile.write(\"%s\\t%s\\t%s\\t%s\\n\" % (_id, question, solution, result))\n",
    "      count += 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "\n",
    "# Create TSVs and get counts.\n",
    "num_nq_examples = {}\n",
    "num_nq_examples['train'] = nq_jsonl_to_tsv(\n",
    "      os.path.join(DATA_DIR, 'hotpot_train_v1.1.json'), nq_tsv_path['train'])\n",
    "num_nq_examples['validation'] = nq_jsonl_to_tsv(\n",
    "      os.path.join(DATA_DIR, 'hotpot_dev_distractor_v1.json'), nq_tsv_path['validation'])\n",
    "\n",
    "json.dump(num_nq_examples, tf.io.gfile.GFile(nq_counts_path, \"w\"))\n",
    "'''\n",
    "\n",
    "\n",
    "def gsm_dataset_fn(split, shuffle_files=False):\n",
    "  \n",
    "  import functools\n",
    "  import os\n",
    "  import time\n",
    "  import warnings\n",
    "  # We only have one file for each split.\n",
    "  del shuffle_files\n",
    "\n",
    "  \n",
    "  examples = get_examples(split)\n",
    "  ids_str = [ex[\"_id\"] for ex in tqdm(examples)]\n",
    "  qns_str = [ex[\"question\"] for ex in tqdm(examples)]\n",
    "  sol_str = [ex[\"solution\"] for ex in tqdm(examples)]\n",
    "  res_str = [ex[\"result\"] for ex in tqdm(examples)]\n",
    "  print(f\"\\n\\n\\n len qns = {len(qns_str)}, len sol = {len(sol_str)}\")\n",
    "  ds = tf.data.Dataset.from_generator(lambda: zip(ids_str, qns_str, sol_str, res_str),\n",
    "                                      output_types=(tf.string, tf.string, tf.string, tf.string),\n",
    "                                      output_shapes=((), (), (), ())\n",
    "                                     )\n",
    "  '''\n",
    "  # Load lines from the text file as examples.\n",
    "  ds = tf.data.TextLineDataset(nq_tsv_path[split])\n",
    "  # Split each \"<question>\\t<answer>\" example into (question, answer) tuple.\n",
    "  ds = ds.map(\n",
    "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\", \"\", \"\"],\n",
    "                        field_delim=\"\\t\", use_quote_delim=False),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  '''\n",
    "  # Map each tuple to a {\"question\": ... \"answer\": ...} dict.\n",
    "  ds = ds.map(lambda *ex: dict(zip([\"id\", \"question\", \"solution\", \"result\"], ex)))\n",
    "  ds = ds.map(lambda ex: {\"id\": ex['id'], \"question\": ex['question'], \"solution\": ex['solution'], \"result\": ex['result']})\n",
    "  return ds\n",
    "\n",
    "\n",
    "@seqio.map_over_dataset\n",
    "def gsm8k(x, include_context=False):\n",
    "  from t5.data.preprocessors import _string_join\n",
    "  \"\"\"Convert GSM examples to a text2text pair.\n",
    "  GSM produces examples with this form:\n",
    "    {'id': <id>, 'question': <problem>, solution': <detailed solution>,\n",
    "     'result': <resulting answer>}\n",
    "  This function will return examples of the format:\n",
    "    {'inputs': 'question: <question> solution: <article>',\n",
    "     'targets': '<result>',\n",
    "     'id': <id>, 'question': <question>, 'solution': <solution>,\n",
    "     'result': <result>},\n",
    "  Args:\n",
    "    x: an example to process.\n",
    "    include_context: a boolean\n",
    "  Returns:\n",
    "    A preprocessed example with the format listed above.\n",
    "  \"\"\"\n",
    "  def _gsm_pad_punctuation(text):\n",
    "    \"\"\"Adds spaces around punctuation.\"\"\"\n",
    "    # Add space around punctuation.\n",
    "    text = tf.strings.regex_replace(text, r'([^0-9A-Za-z#_])', r' \\1 ')\n",
    "    # Collapse consecutive whitespace into one space.\n",
    "    text = tf.strings.regex_replace(text, r'\\s+', ' ')\n",
    "    return text\n",
    "  \n",
    "  print(f\"\\n\\n\\nx keys {x.keys()}\")\n",
    "  q = _gsm_pad_punctuation(x['question'])\n",
    "  s = _gsm_pad_punctuation(x['solution'])\n",
    "  r = _gsm_pad_punctuation(x['result'])\n",
    "  if include_context:\n",
    "    inputs = _string_join(['question:', q, 'solution:', s])\n",
    "    targets = r\n",
    "  else:\n",
    "    inputs = _string_join(['question:', q])\n",
    "    targets = s + r\n",
    "  return {\n",
    "      'inputs': inputs,\n",
    "      'targets': targets,\n",
    "      'id': x['id'],\n",
    "      'question': q,\n",
    "      'solution': s,\n",
    "      'result': r\n",
    "  }\n",
    "\n",
    "#print(\"A few raw validation examples...\")\n",
    "#for ex in tfds.as_numpy(nq_dataset_fn(\"train\").take(5)):\n",
    "#  print(ex)\n",
    "\n",
    "'''\n",
    "def trivia_preprocessor(ds):\n",
    "  def normalize_text(text):\n",
    "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
    "    return text\n",
    "\n",
    "  def to_inputs_and_targets(ex):\n",
    "    \"\"\"Map {\"question\": ..., \"answer\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
    "    return {\n",
    "        \"inputs\":\n",
    "             tf.strings.join(#'question: <question> context: <article>'\n",
    "                 [\"question: \", normalize_text(ex[\"question\"]),\"context: \", normalize_text(ex[\"context\"])]),\n",
    "        \"targets\": normalize_text(ex[\"answer\"])\n",
    "    }\n",
    "  return ds.map(to_inputs_and_targets, \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                \n",
    "\n",
    "\n",
    "from create_hotpotqa_task import nq_dataset_fn, num_nq_examples, trivia_preprocessor\n",
    "t5.data.TaskRegistry.add(\n",
    "    \"hotpotqa_context\",\n",
    "    # Specify the task type.\n",
    "    t5.data.Task,\n",
    "    # Supply a function which returns a tf.data.Dataset.\n",
    "    dataset_fn=nq_dataset_fn,\n",
    "    splits=[\"train\"],\n",
    "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
    "    text_preprocessor=[trivia_preprocessor],\n",
    "    # Lowercase targets before computing metrics.\n",
    "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
    "    # We'll use accuracy as our evaluation metric.\n",
    "    metric_fns=[t5.evaluation.metrics.accuracy]#,\n",
    "    # Not required, but helps for mixing and auto-caching.\n",
    "    #num_input_examples=num_nq_examples\n",
    ")\n",
    "\n",
    "nq_task = t5.data.TaskRegistry.get(\"hotpotqa_context\")\n",
    "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 128, \"targets\": 32})\n",
    "'''\n",
    "#print(\"A few preprocessed validation examples...\")\n",
    "#for ex in tfds.as_numpy(ds.take(5)):\n",
    "#  print(ex)\n",
    "def test_solve_rate(targets, predictions):\n",
    "  \"\"\"Computes SQuAD metrics, maximizing over answers per question.\n",
    "  Args:\n",
    "    targets: list of lists of results\n",
    "    predictions: list of predicted solutions\n",
    "  Returns:\n",
    "    dict with score_key: squad score across all targets and predictions\n",
    "  \"\"\"\n",
    "  targets = [qa_utils.normalize_squad(t)for t in targets]\n",
    "  predictions = [qa_utils.normalize_squad(p) for p in predictions]\n",
    "  \n",
    "  if len(targets) != len(predictions):\n",
    "    raise ValueError(\"Number of targets and predictions must match.\")\n",
    "  overall = len(targets)\n",
    "  c = 0\n",
    "  for t, p in zip(targets, predictions):\n",
    "    \n",
    "    # t5.data.get_default_vocabulary().decode([3, 30345,30345]) = \"####\"\n",
    "    final_answer_start = tf.where(t == 30345)[-1][0]\n",
    "    final_answer_end = tf.where(t == 1)[0][0]\n",
    "    real_res = t[final_answer_start+1:final_answer_end]\n",
    "  \n",
    "    \n",
    "    zz = tf.where(p == 30345)\n",
    "    if zz:\n",
    "      pred_answer_start = zz[-1][0]\n",
    "      ozzy = tf.where(p == 1)\n",
    "      if ozzy:\n",
    "        pred_answer_end = ozzy[0][0]\n",
    "        pred_res = p[pred_answer_start+1:pred_answer_end]\n",
    "        c += int(real_res == pred_res)\n",
    "      else:\n",
    "        logging.info(f'/n/npred answer has no eos token: {p}/n/n')\n",
    "    else:\n",
    "        logging.info(f'/n/npred answer has no #### token: {p}/n/n')\n",
    "        \n",
    "  \n",
    "  tsr = c / overall\n",
    "  tsr *= 100\n",
    "  logging.info(\"Test Solve Rate = %.2f\", tsr)\n",
    "  return {\"tsr\": tsr}\n",
    "\n",
    "\n",
    "import functools\n",
    "t5.data.TaskRegistry.add(\n",
    "    \"gsm8k_baseline1\",\n",
    "    # Specify the task type.\n",
    "    t5.data.Task,\n",
    "    # Supply a function which returns a tf.data.Dataset.\n",
    "    dataset_fn=gsm_dataset_fn,\n",
    "    splits=[\"train\", \"test\"],\n",
    "    text_preprocessor=[\n",
    "        functools.partial(gsm8k, include_context=True),\n",
    "    ],\n",
    "    postprocess_fn=t5.data.postprocessors.qa,\n",
    "    metric_fns=[t5.evaluation.metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "    \n",
    "t5.data.TaskRegistry.add(\n",
    "    \"gsm8k_wm1\",\n",
    "    # Specify the task type.\n",
    "    t5.data.Task,\n",
    "    # Supply a function which returns a tf.data.Dataset.\n",
    "    dataset_fn=gsm_dataset_fn,\n",
    "    splits=[\"train\", \"test\"],\n",
    "    text_preprocessor=[\n",
    "        functools.partial(gsm8k, include_context=False),\n",
    "    ],\n",
    "    postprocess_fn=t5.data.postprocessors.qa,\n",
    "    metric_fns=[t5.evaluation.metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473 train examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7473/7473 [00:00<00:00, 2126460.91it/s]\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 2038636.34it/s]\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 1979664.86it/s]\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 2431843.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " len qns = 7473, len sol = 7473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "x keys dict_keys(['id', 'question', 'solution', 'result'])\n"
     ]
    }
   ],
   "source": [
    "nq_task = t5.data.TaskRegistry.get(\"gsm8k_wm1\")\n",
    "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 32})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473 train examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7473/7473 [00:00<00:00, 2152749.57it/s]\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 1928982.32it/s]\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 1863830.28it/s]\n",
      "100%|██████████| 7473/7473 [00:00<00:00, 2359888.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " len qns = 7473, len sol = 7473\n",
      "\n",
      "\n",
      "\n",
      "x keys dict_keys(['id', 'question', 'solution', 'result'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 258 418\n",
      "1319 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1319/1319 [00:00<00:00, 1061044.68it/s]\n",
      "100%|██████████| 1319/1319 [00:00<00:00, 1673914.36it/s]\n",
      "100%|██████████| 1319/1319 [00:00<00:00, 1680014.27it/s]\n",
      "100%|██████████| 1319/1319 [00:00<00:00, 1777727.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " len qns = 1319, len sol = 1319\n",
      "\n",
      "\n",
      "\n",
      "x keys dict_keys(['id', 'question', 'solution', 'result'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 208 389\n"
     ]
    }
   ],
   "source": [
    "ds = t5.data.TaskRegistry.get(\"gsm8k_wm1\").get_dataset(split=\"train\", sequence_length={\"inputs\": 1024, \"targets\": 1024})\n",
    "print(sum([len(i['inputs']) > 256 for i in ds]), max([len(i['inputs']) for i in ds]), max([len(i['targets']) for i in ds]))\n",
    "\n",
    "ds = t5.data.TaskRegistry.get(\"gsm8k_wm1\").get_dataset(split=\"test\", sequence_length={\"inputs\": 1024, \"targets\": 1024})\n",
    "print(sum([len(i['inputs']) > 512 for i in ds]), max([len(i['inputs']) for i in ds]), max([len(i['targets']) for i in ds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supporting_facts\n",
      "[[\"Arthur's Magazine\", 0], ['First for Women', 0]]\n",
      "\n",
      "\n",
      "level\n",
      "medium\n",
      "\n",
      "\n",
      "question\n",
      "Which magazine was started first Arthur's Magazine or First for Women?\n",
      "\n",
      "\n",
      "context\n",
      "[['Radio City (Indian radio station)', [\"Radio City is India's first private FM radio station and was started on 3 July 2001.\", ' It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).', ' It plays Hindi, English and regional songs.', ' It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.', ' Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.', ' The Radio station currently plays a mix of Hindi and Regional music.', ' Abraham Thomas is the CEO of the company.']], ['History of Albanian football', ['Football in Albania existed before the Albanian Football Federation (FSHF) was created.', \" This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) .\", ' Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play its first international match and then defeated Yugoslavia in 1946.', ' In 1932, Albania joined FIFA (during the 12–16 June convention ) And in 1954 she was one of the founding members of UEFA.']], ['Echosmith', ['Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California.', ' Originally formed as a quartet of siblings, the band currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016.', ' Echosmith started first as \"Ready Set Go!\"', ' until they signed to Warner Bros.', ' Records in May 2012.', ' They are best known for their hit song \"Cool Kids\", which reached number 13 on the \"Billboard\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia.', ' The song was Warner Bros.', \" Records' fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold.\", ' The band\\'s debut album, \"Talking Dreams\", was released on October 8, 2013.']], [\"Women's colleges in the Southern United States\", [\"Women's colleges in the Southern United States refers to undergraduate, bachelor's degree–granting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States.\", \" Many started first as girls' seminaries or academies.\", ' Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women.', ' Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level.']], ['First Arthur County Courthouse and Jail', ['The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum.']], [\"Arthur's Magazine\", [\"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.\", ' Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.', ' In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\".']], ['2014–15 Ukrainian Hockey Championship', ['The 2014–15 Ukrainian Hockey Championship was the 23rd season of the Ukrainian Hockey Championship.', ' Only four teams participated in the league this season, because of the instability in Ukraine and that most of the clubs had economical issues.', ' Generals Kiev was the only team that participated in the league the previous season, and the season started first after the year-end of 2014.', ' The regular season included just 12 rounds, where all the teams went to the semifinals.', ' In the final, ATEK Kiev defeated the regular season winner HK Kremenchuk.']], ['First for Women', [\"First for Women is a woman's magazine published by Bauer Media Group in the USA.\", ' The magazine was started in 1989.', ' It is based in Englewood Cliffs, New Jersey.', ' In 2011 the circulation of the magazine was 1,310,696 copies.']], ['Freeway Complex Fire', ['The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California.', ' The fire started as two separate fires on November 15, 2008.', ' The \"Freeway Fire\" started first shortly after 9am with the \"Landfill Fire\" igniting approximately 2 hours later.', ' These two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda.']], ['William Rast', ['William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala.', ' It is most known for their premium jeans.', ' On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line.', ' The label also produces other clothing items such as jackets and tops.', ' The company started first as a denim line, later evolving into a men’s and women’s clothing line.']]]\n",
      "\n",
      "\n",
      "answer\n",
      "Arthur's Magazine\n",
      "\n",
      "\n",
      "_id\n",
      "5a7a06935542990198eaf050\n",
      "\n",
      "\n",
      "type\n",
      "comparison\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for k,v in data[0].items():\n",
    "  print(k)\n",
    "  print(v)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/asagirova/hotpotqa'\n",
    "with open(os.path.join(DATA_DIR, 'hotpot_train_v1.1.json'), \"rb\") as infile:\n",
    "    for line in infile:\n",
    "      ex = json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['supporting_facts', 'level', 'question', 'context', 'answer', '_id', 'type'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arthur's Magazine\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/asagirova/hotpotqa'\n",
    "nq_counts_path = os.path.join(DATA_DIR, \"hotpotqa-counts.json\")\n",
    "nq_tsv_path = {\n",
    "    \"train\": os.path.join(DATA_DIR, \"hotpotqa-train.tsv\"),\n",
    "    \"validation\": os.path.join(DATA_DIR, \"hotpotqa-val.tsv\")\n",
    "}\n",
    "\n",
    "def nq_jsonl_to_tsv(in_fname, out_fname):\n",
    "\n",
    "  \n",
    "  count = 0\n",
    "  with open(in_fname, \"rb\") as infile,\\\n",
    "       open(out_fname, \"w\") as outfile:\n",
    "    data = json.load(infile)\n",
    "    for ex in data:\n",
    "      \n",
    "      # Questions in NQ do not include a question mark.\n",
    "      question = ex[\"question\"]\n",
    "      answer = ex['answer']\n",
    "      # Handle the two document formats in NQ (tokens or text).\n",
    "      if \"document_tokens\" in ex:\n",
    "        tokens = [t[\"token\"] for t in ex[\"document_tokens\"]]\n",
    "      elif \"document_text\" in ex:\n",
    "        tokens = ex[\"document_text\"].split(\" \")\n",
    "      answer = extract_answer(tokens, answer_span)\n",
    "      # Write this line as <question>\\t<answer>\n",
    "      outfile.write(\"%s\\t%s\\n\" % (question, answer))\n",
    "      count += 1\n",
    "      tf.logging.log_every_n(\n",
    "          tf.logging.INFO,\n",
    "          \"Wrote %d examples to %s.\" % (count, out_fname),\n",
    "          1000)\n",
    "    return count\n",
    "\n",
    "\n",
    "# Create TSVs and get counts.\n",
    "num_nq_examples = {}\n",
    "num_nq_examples['tain'] = nq_jsonl_to_tsv(\n",
    "      os.path.join(DATA_DIR, 'hotpot_train_v1.1.json'), nq_tsv_path['train'])\n",
    "json.dump(num_nq_examples, tf.io.gfile.GFile(nq_counts_path, \"w\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5-conda",
   "language": "python",
   "name": "t5-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
